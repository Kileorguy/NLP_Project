{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ur1S_a43pwaI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import string\n",
        "import math\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Sjvf3o2DK5IX",
        "outputId": "1197789e-4529-4dae-e6e2-468c6f8f758c"
      },
      "outputs": [],
      "source": [
        "# !pip install pyenchant\n",
        "# !apt-get install -y libenchant-2-2\n",
        "import enchant\n",
        "from enchant.tokenize import get_tokenizer\n",
        "from enchant.tokenize import basic_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VyDxTbTbiyED",
        "outputId": "de2f280d-0f74-45e9-d9ac-3cdbeb0fbc9c"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow\n",
        "from tensorflow.keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "HH3Q8DBiD5HQ",
        "outputId": "a088728d-f0d3-4570-a6cd-967bfb6f578e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n  NOTE WORD2VEC\\n\\n  1. remove punctuation\\n  2. apply word2vec\\n\\n  vector_size:\\n      Defines the length of the vector representations for each word.\\n      window: The size of the context window around the target word.\\n      min_count: Ignores words with a total frequency lower than this threshold.\\n      workers: Number of CPU cores to use for training.\\n\\n  note: bisa juga ambil yang pre-trained\\n  '"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "dataset = './dataset/ielts-writing-essays.csv'\n",
        "\n",
        "\"\"\"\n",
        "  NOTE WORD2VEC\n",
        "\n",
        "  1. remove punctuation\n",
        "  2. apply word2vec\n",
        "\n",
        "  vector_size:\n",
        "      Defines the length of the vector representations for each word.\n",
        "      window: The size of the context window around the target word.\n",
        "      min_count: Ignores words with a total frequency lower than this threshold.\n",
        "      workers: Number of CPU cores to use for training.\n",
        "\n",
        "  note: bisa juga ambil yang pre-trained\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx0erCPFj9Bg",
        "outputId": "94f2269e-118a-4a9f-bbe3-8dce25c4fe37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               Essay  Overall\n",
            "0  Between 1995 and 2010, a study was conducted r...      5.5\n",
            "1  Poverty represents a worldwide crisis. It is t...      6.5\n",
            "2  The left chart shows the population change hap...      5.0\n",
            "3  Human beings are facing many challenges nowada...      5.5\n",
            "4  Information about the thousands of visits from...      7.0\n"
          ]
        }
      ],
      "source": [
        "# Load dataset & Prepare Dataframes\n",
        "df = pd.read_csv(dataset)\n",
        "print(df.head())\n",
        "sentences = pd.DataFrame(df['Essay'])\n",
        "scores = pd.DataFrame(df['Overall'])\n",
        "\n",
        "# Define functions\n",
        "def tokenize_and_filter_sentence(sentence):\n",
        "    tokenizer = enchant.tokenize.get_tokenizer(\"en_GB\")\n",
        "    tokens = list(tokenizer(sentence))\n",
        "    tokens = [token[0] for token in tokens]\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "def convert_to_sequence(sentence, word_index):\n",
        "    tokens = tokenize_and_filter_sentence(sentence)\n",
        "    return [word_index.get(token, word_index['<OOV>']) for token in tokens]\n",
        "\n",
        "def alternative_word2vec(sen, model):\n",
        "    return [model.wv[word] for word in sen if word in model.wv]\n",
        "\n",
        "def calculate_average_length(sentences):\n",
        "    total_len = 0\n",
        "    for sentence in sentences:\n",
        "        total_len += len(sentence)\n",
        "    average_len = int(round(total_len / len(sentences), 0))\n",
        "    return average_len\n",
        "\n",
        "def pad_word2vec(sentence_vectors, maxlen, vector_size):\n",
        "    sentence_length = len(sentence_vectors)\n",
        "    if sentence_length > maxlen:\n",
        "        return np.array(sentence_vectors[:maxlen])\n",
        "    else:\n",
        "        padding = [np.zeros(vector_size) for _ in range(maxlen - sentence_length)]\n",
        "        return np.array(sentence_vectors + padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padded Sequences: [[    0     0     0 ...   716   560 12498]\n",
            " [ 9296 10281     1 ...  9495 12254  5637]\n",
            " [    0     0     0 ... 12260  8800 13625]\n",
            " [ 5994  1172   714 ... 11948  2242  4847]\n",
            " [    0     0     0 ...  8365 12260  8918]]\n",
            "Word2Vec Data Shape: (1435, 253, 100)\n"
          ]
        }
      ],
      "source": [
        "# Preprocess Data\n",
        "tokenized_sentences = sentences['Essay'].apply(tokenize_and_filter_sentence)\n",
        "all_sentences = sentences['Essay']\n",
        "all_tokens = [token for sentence in all_sentences for token in tokenize_and_filter_sentence(sentence)]\n",
        "unique_tokens = sorted(set(all_tokens))\n",
        "word_index = {word: idx + 1 for idx, word in enumerate(unique_tokens)}\n",
        "word_index['<OOV>'] = len(word_index) + 1\n",
        "sequences = [convert_to_sequence(sentence, word_index) for sentence in all_sentences]\n",
        "\n",
        "vector_size = 100\n",
        "maxlen = calculate_average_length(tokenized_sentences)\n",
        "padding_type = 'pre'\n",
        "truncating_type = 'post'\n",
        "padded_sequences = pad_sequences(\n",
        "    sequences,\n",
        "    truncating = truncating_type,\n",
        "    padding = padding_type,\n",
        "    maxlen = maxlen,\n",
        "    dtype='int32'\n",
        ")\n",
        "\n",
        "print(\"Padded Sequences:\", padded_sequences[:5]) #--> use this for training the first LSTM for Label Encoded Data\n",
        "\n",
        "sentences_WV = sentences.copy()\n",
        "tokenized_sentences_list = tokenized_sentences.tolist()\n",
        "model = Word2Vec(tokenized_sentences_list, vector_size=100, window=5, min_count=1, workers=4)\n",
        "sentences_WV['Essay'] = sentences_WV['Essay'].apply(lambda x: alternative_word2vec(x, model))\n",
        "\n",
        "vector_size = 100\n",
        "maxlen = calculate_average_length(tokenized_sentences)\n",
        "sentences_WV['Padded_Essay'] = sentences_WV['Essay'].apply(lambda x: pad_word2vec(x, maxlen, vector_size))\n",
        "\n",
        "word2vec_data = np.array(sentences_WV['Padded_Essay'].tolist())\n",
        "print(\"Word2Vec Data Shape:\", word2vec_data.shape) #--> use this for training the second LSTM for Word Vectorized Data\n",
        "\n",
        "trainX_labelEncoded = torch.tensor(padded_sequences[:, :, None], dtype=torch.float32)\n",
        "trainY_labelEncoded = torch.tensor(scores.values, dtype=torch.float32)[:, None]\n",
        "\n",
        "trainX_word2Vec = torch.tensor(word2vec_data, dtype=torch.float32)\n",
        "trainY_word2Vec = torch.tensor(scores.values, dtype=torch.float32)[:, None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "M6HKoRxvic37"
      },
      "outputs": [],
      "source": [
        "class LSTMModelWord2Vec(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_layers, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim, \n",
        "            hidden_dim, \n",
        "            num_layers=num_layers, \n",
        "            dropout=dropout_rate, \n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # Output is a single value (regression)\n",
        "        \n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, src, hidden):\n",
        "        output, hidden = self.lstm(src, hidden)\n",
        "        output = self.dropout(output[:, -1, :])  # Use the last output\n",
        "        prediction = self.fc(output)\n",
        "        return prediction, hidden\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_range = 1 / math.sqrt(self.hidden_dim)\n",
        "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return hidden, cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PARAMS\n",
        "vocab_size = len(unique_tokens)\n",
        "embedding_dim = 256             # 400 in the paper\n",
        "hidden_dim = 256                # 1150 in the paper\n",
        "num_layers = 3                   # 3 in the paper\n",
        "dropout_rate = 0.2                            \n",
        "lr = 1e-3\n",
        "input_layer = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJLEtd27sQIA",
        "outputId": "fddc2c7e-3173-4f5d-d112-f596e5340fc8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "g:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([32, 1, 1])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected hidden[0] size (3, 27, 256), got [3, 32, 256]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[56], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m hidden, cell \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39mdetach(), cell\u001b[38;5;241m.\u001b[39mdetach()  \u001b[38;5;66;03m# Detach hidden states\u001b[39;00m\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 29\u001b[0m predictions, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_wv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, targets)\n\u001b[0;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[55], line 19\u001b[0m, in \u001b[0;36mLSTMModelWord2Vec.forward\u001b[1;34m(self, src, hidden)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, hidden):\n\u001b[1;32m---> 19\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# Use the last output\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(output)\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1119\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1116\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   1117\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m-> 1119\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1001\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    997\u001b[0m     hidden: Tuple[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     batch_sizes: Optional[Tensor],\n\u001b[0;32m    999\u001b[0m ):\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m-> 1001\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_hidden_size\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_expected_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExpected hidden[0] size \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, got \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[0;32m   1007\u001b[0m         hidden[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   1008\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m   1009\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1010\u001b[0m     )\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:345\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_hidden_size\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     hx: Tensor,\n\u001b[0;32m    341\u001b[0m     expected_hidden_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    342\u001b[0m     msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    343\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m expected_hidden_size:\n\u001b[1;32m--> 345\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(expected_hidden_size, \u001b[38;5;28mlist\u001b[39m(hx\u001b[38;5;241m.\u001b[39msize())))\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden[0] size (3, 27, 256), got [3, 32, 256]"
          ]
        }
      ],
      "source": [
        "# Data Loader\n",
        "batch_size = 32\n",
        "dataset = torch.utils.data.TensorDataset(trainX_word2Vec, trainY_word2Vec)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Model Initialization\n",
        "embedding_dim = vector_size  # Word2Vec vector size\n",
        "hidden_dim = 256\n",
        "num_layers = 3\n",
        "dropout_rate = 0.2\n",
        "lr = 1e-3\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_wv = LSTMModelWord2Vec(embedding_dim, hidden_dim, num_layers, dropout_rate).to(device)\n",
        "optimizer = torch.optim.Adam(model_wv.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()  # Regression loss\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model_wv.train()\n",
        "    epoch_loss = 0\n",
        "    hidden, cell = model_wv.init_hidden(batch_size, device)\n",
        "\n",
        "    for inputs, targets in data_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        hidden, cell = hidden.detach(), cell.detach()  # Detach hidden states\n",
        "        optimizer.zero_grad()\n",
        "        predictions, (hidden, cell) = model_wv(inputs, (hidden, cell))\n",
        "        loss = criterion(predictions, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(data_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
