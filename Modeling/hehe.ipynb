{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import enchant\n",
    "from enchant.tokenize import get_tokenizer\n",
    "from enchant.tokenize import basic_tokenize\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/ielts-writing-essays.csv')\n",
    "df.head(5)\n",
    "\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len Essays Token :1435\n",
      "Len All Token :359245\n",
      "Len All Vocab :13350\n"
     ]
    }
   ],
   "source": [
    "essay = df['Essay'].to_list()\n",
    "\n",
    "essays_token = []\n",
    "all_token = []\n",
    "\n",
    "for i in essay:\n",
    "    token = []\n",
    "    words = word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in punctuation and word.isalpha():\n",
    "            word = word.lower()\n",
    "            all_token.append(word)\n",
    "        \n",
    "            token.append(word)\n",
    "    essays_token.append(token)\n",
    "all_vocab = list(set(all_token))\n",
    "all_vocab = ['<unk>'] + all_vocab\n",
    "label_encoder.fit(all_vocab)\n",
    "\n",
    "with open(\"label_encoder.pkl\", \"wb\") as file:\n",
    "    pickle.dump(label_encoder, file)\n",
    "\n",
    "print(f\"Len Essays Token :{len(essays_token)}\")\n",
    "print(f\"Len All Token :{len(all_token)}\")\n",
    "print(f\"Len All Vocab :{len(all_vocab)}\")\n",
    "SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_length(sentences):\n",
    "    total_len = 0\n",
    "    for sentence in sentences:\n",
    "        total_len += len(sentence)\n",
    "    average_len = int(round(total_len / len(sentences), 0))\n",
    "    return average_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, batch_size):\n",
    "    \"\"\"Convert tokenized dataset to tensor batches.\"\"\"\n",
    "    data = [torch.LongTensor(label_encoder.transform(tokens)) for tokens in dataset]\n",
    "    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.view(batch_size, -1) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004\n",
      "215\n"
     ]
    }
   ],
   "source": [
    "train_len = round(len(essays_token) * 0.7)\n",
    "test_val_len = round(len(essays_token) * 0.15)\n",
    "\n",
    "train_data = essays_token[:train_len]\n",
    "test_data = essays_token[train_len: train_len + test_val_len]\n",
    "val_data = essays_token[train_len + test_val_len:]\n",
    "# 0.7 0.15\n",
    "\n",
    "batch_size = 16\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "train_data = get_data(train_data,batch_size)\n",
    "test_data = get_data(test_data,batch_size)\n",
    "val_data = get_data(val_data,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1210,   553,     1,  ...,     0,     0,     0],\n",
      "        [  103, 12133, 11974,  ...,     0,     0,     0],\n",
      "        [12133,  7822,  5780,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [12016, 12411,  5410,  ...,     0,     0,     0],\n",
      "        [ 6625, 10862, 10804,  ...,     0,     0,     0],\n",
      "        [12046,  5407, 10804,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate):\n",
    "                \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        output, hidden = self.lstm(embedding, hidden)          \n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden\n",
    "    \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hidden_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(all_vocab)\n",
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 128       \n",
    "NUM_LAYERS = 3   \n",
    "DROPOUT_RATE = 0.75\n",
    "lr = 1e-3\n",
    "EPOCHS = 50\n",
    "clip = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,601,574 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LSTM(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT_RATE).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    src = data[:, idx:idx + seq_len]\n",
    "    tgt = data[:, idx + 1:idx + seq_len + 1]\n",
    "    return src, tgt\n",
    "\n",
    "def train_epoch(model, data, optimizer, criterion, batch_size, seq_len, clip,device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = data.shape[1] // seq_len\n",
    "    hidden = model.init_hidden(batch_size,device)\n",
    "\n",
    "    for idx in tqdm(range(0, num_batches * seq_len, seq_len), desc='Training', leave=False):\n",
    "        src, tgt = get_batch(data, seq_len, idx)\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "        predictions, hidden = model(src, hidden)\n",
    "\n",
    "        predictions = predictions.reshape(-1, VOCAB_SIZE)\n",
    "        tgt = tgt.reshape(-1)\n",
    "        loss = criterion(predictions, tgt)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / num_batches\n",
    "\n",
    "def evaluate_epoch(model, data, criterion, batch_size, seq_len,device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    num_batches = data.shape[1] // seq_len\n",
    "    hidden = model.init_hidden(batch_size,device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches * seq_len, seq_len):\n",
    "            src, tgt = get_batch(data, seq_len, idx)\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            predictions, hidden = model(src, hidden)\n",
    "\n",
    "            predictions = predictions.reshape(-1, VOCAB_SIZE)\n",
    "            tgt = tgt.reshape(-1)\n",
    "            loss = criterion(predictions, tgt)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Loss: 3.7256 | Train Perplexity: 41.498\n",
      "Valid Loss: 3.7468 | Validation Perplexity: 42.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Train Loss: 2.8289 | Train Perplexity: 16.927\n",
      "Valid Loss: 3.6988 | Validation Perplexity: 40.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Train Loss: 2.7831 | Train Perplexity: 16.169\n",
      "Valid Loss: 3.6737 | Validation Perplexity: 39.396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Train Loss: 2.7547 | Train Perplexity: 15.716\n",
      "Valid Loss: 3.6574 | Validation Perplexity: 38.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Train Loss: 2.7281 | Train Perplexity: 15.304\n",
      "Valid Loss: 3.6435 | Validation Perplexity: 38.224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Train Loss: 2.7063 | Train Perplexity: 14.974\n",
      "Valid Loss: 3.6333 | Validation Perplexity: 37.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "Train Loss: 2.6870 | Train Perplexity: 14.687\n",
      "Valid Loss: 3.6228 | Validation Perplexity: 37.443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "Train Loss: 2.6672 | Train Perplexity: 14.399\n",
      "Valid Loss: 3.6167 | Validation Perplexity: 37.216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "Train Loss: 2.6517 | Train Perplexity: 14.177\n",
      "Valid Loss: 3.6103 | Validation Perplexity: 36.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "Train Loss: 2.6356 | Train Perplexity: 13.951\n",
      "Valid Loss: 3.6056 | Validation Perplexity: 36.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:\n",
      "Train Loss: 2.6194 | Train Perplexity: 13.728\n",
      "Valid Loss: 3.6030 | Validation Perplexity: 36.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:\n",
      "Train Loss: 2.6049 | Train Perplexity: 13.530\n",
      "Valid Loss: 3.5949 | Validation Perplexity: 36.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:\n",
      "Train Loss: 2.5929 | Train Perplexity: 13.368\n",
      "Valid Loss: 3.5863 | Validation Perplexity: 36.102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:\n",
      "Train Loss: 2.5786 | Train Perplexity: 13.179\n",
      "Valid Loss: 3.5780 | Validation Perplexity: 35.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:\n",
      "Train Loss: 2.5666 | Train Perplexity: 13.022\n",
      "Valid Loss: 3.5732 | Validation Perplexity: 35.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:\n",
      "Train Loss: 2.5559 | Train Perplexity: 12.883\n",
      "Valid Loss: 3.5706 | Validation Perplexity: 35.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:\n",
      "Train Loss: 2.5442 | Train Perplexity: 12.733\n",
      "Valid Loss: 3.5637 | Validation Perplexity: 35.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:\n",
      "Train Loss: 2.5324 | Train Perplexity: 12.584\n",
      "Valid Loss: 3.5560 | Validation Perplexity: 35.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:\n",
      "Train Loss: 2.5248 | Train Perplexity: 12.488\n",
      "Valid Loss: 3.5580 | Validation Perplexity: 35.092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:\n",
      "Train Loss: 2.5113 | Train Perplexity: 12.321\n",
      "Valid Loss: 3.5570 | Validation Perplexity: 35.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:\n",
      "Train Loss: 2.5053 | Train Perplexity: 12.248\n",
      "Valid Loss: 3.5545 | Validation Perplexity: 34.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:\n",
      "Train Loss: 2.5019 | Train Perplexity: 12.206\n",
      "Valid Loss: 3.5572 | Validation Perplexity: 35.065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:\n",
      "Train Loss: 2.4997 | Train Perplexity: 12.179\n",
      "Valid Loss: 3.5491 | Validation Perplexity: 34.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:\n",
      "Train Loss: 2.4982 | Train Perplexity: 12.160\n",
      "Valid Loss: 3.5502 | Validation Perplexity: 34.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:\n",
      "Train Loss: 2.4990 | Train Perplexity: 12.171\n",
      "Valid Loss: 3.5444 | Validation Perplexity: 34.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26:\n",
      "Train Loss: 2.4976 | Train Perplexity: 12.153\n",
      "Valid Loss: 3.5439 | Validation Perplexity: 34.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27:\n",
      "Train Loss: 2.4961 | Train Perplexity: 12.135\n",
      "Valid Loss: 3.5432 | Validation Perplexity: 34.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28:\n",
      "Train Loss: 2.4963 | Train Perplexity: 12.138\n",
      "Valid Loss: 3.5440 | Validation Perplexity: 34.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29:\n",
      "Train Loss: 2.4958 | Train Perplexity: 12.132\n",
      "Valid Loss: 3.5413 | Validation Perplexity: 34.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:\n",
      "Train Loss: 2.4962 | Train Perplexity: 12.136\n",
      "Valid Loss: 3.5414 | Validation Perplexity: 34.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31:\n",
      "Train Loss: 2.4952 | Train Perplexity: 12.125\n",
      "Valid Loss: 3.5416 | Validation Perplexity: 34.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32:\n",
      "Train Loss: 2.4962 | Train Perplexity: 12.137\n",
      "Valid Loss: 3.5418 | Validation Perplexity: 34.530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33:\n",
      "Train Loss: 2.4957 | Train Perplexity: 12.130\n",
      "Valid Loss: 3.5418 | Validation Perplexity: 34.530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34:\n",
      "Train Loss: 2.4956 | Train Perplexity: 12.129\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:\n",
      "Train Loss: 2.4948 | Train Perplexity: 12.120\n",
      "Valid Loss: 3.5421 | Validation Perplexity: 34.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36:\n",
      "Train Loss: 2.4970 | Train Perplexity: 12.146\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37:\n",
      "Train Loss: 2.4959 | Train Perplexity: 12.132\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38:\n",
      "Train Loss: 2.4953 | Train Perplexity: 12.125\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39:\n",
      "Train Loss: 2.4955 | Train Perplexity: 12.128\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40:\n",
      "Train Loss: 2.4952 | Train Perplexity: 12.124\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41:\n",
      "Train Loss: 2.4953 | Train Perplexity: 12.125\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42:\n",
      "Train Loss: 2.4951 | Train Perplexity: 12.122\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43:\n",
      "Train Loss: 2.4957 | Train Perplexity: 12.130\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44:\n",
      "Train Loss: 2.4966 | Train Perplexity: 12.141\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45:\n",
      "Train Loss: 2.4962 | Train Perplexity: 12.136\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46:\n",
      "Train Loss: 2.4961 | Train Perplexity: 12.135\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47:\n",
      "Train Loss: 2.4956 | Train Perplexity: 12.128\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48:\n",
      "Train Loss: 2.4945 | Train Perplexity: 12.116\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49:\n",
      "Train Loss: 2.4946 | Train Perplexity: 12.117\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50:\n",
      "Train Loss: 2.4958 | Train Perplexity: 12.131\n",
      "Valid Loss: 3.5420 | Validation Perplexity: 34.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Win10\\AppData\\Local\\Temp\\ipykernel_19048\\1174497199.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best-val-lstm_lm.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.9507 | Test Perplexity: 19.120\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(model, train_data, optimizer, criterion, batch_size, SEQ_LEN, clip,device)\n",
    "    valid_loss = evaluate_epoch(model, val_data, criterion, batch_size, SEQ_LEN,device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Perplexity: {math.exp(train_loss):.3f}\")\n",
    "    print(f\"Valid Loss: {valid_loss:.4f} | Validation Perplexity: {math.exp(valid_loss):.3f}\")\n",
    "\n",
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt'))\n",
    "test_loss = evaluate_epoch(model, test_data, criterion, batch_size, SEQ_LEN,device)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Perplexity: {math.exp(test_loss):.3f}\")\n",
    "\n",
    "torch.save(model, \"entire_model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, seq_len, vocab, label_encoder, device, max_length=50):\n",
    "    tokens = word_tokenize(prompt)\n",
    "    token_indices = label_encoder.transform(tokens)\n",
    "    \n",
    "    input_tensor = torch.LongTensor(token_indices).unsqueeze(0).to(device)  \n",
    "    \n",
    "    hidden = model.init_hidden(1,device)\n",
    "    \n",
    "    # Generate text\n",
    "    model.eval()\n",
    "    generated_text = tokens.copy()  \n",
    "    for _ in range(max_length):\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            predictions, hidden = model(input_tensor, hidden)\n",
    "        predicted_idx = predictions[0, -1].argmax().item()\n",
    "        if predicted_idx == 0:  \n",
    "            continue\n",
    "        predicted_word = label_encoder.inverse_transform([predicted_idx])[0]\n",
    "        \n",
    "        generated_text.append(predicted_word)\n",
    "        \n",
    "        input_tensor = torch.cat((input_tensor, torch.LongTensor([predicted_idx]).unsqueeze(0).to(device)), dim=1)\n",
    "    \n",
    "    generated_text_str = ' '.join(generated_text)\n",
    "    return generated_text_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: school and the other hand the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prompt = \"school\"\n",
    "generated_text = generate_text(model, prompt, SEQ_LEN, all_vocab, label_encoder, device, max_length=100)\n",
    "print(f\"Generated text: {generated_text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
