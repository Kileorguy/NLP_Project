{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ur1S_a43pwaI"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[WinError 127] The specified procedure could not be found",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\torchtext\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\torchtext\\_extension.py:64\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\torch\\_ops.py:1350\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1345\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m-> 1350\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
            "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import string\n",
        "import math\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Sjvf3o2DK5IX",
        "outputId": "1197789e-4529-4dae-e6e2-468c6f8f758c"
      },
      "outputs": [],
      "source": [
        "# !pip install pyenchant\n",
        "# !apt-get install -y libenchant-2-2\n",
        "import enchant\n",
        "from enchant.tokenize import get_tokenizer\n",
        "from enchant.tokenize import basic_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VyDxTbTbiyED",
        "outputId": "de2f280d-0f74-45e9-d9ac-3cdbeb0fbc9c"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow\n",
        "from tensorflow.keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "HH3Q8DBiD5HQ",
        "outputId": "a088728d-f0d3-4570-a6cd-967bfb6f578e"
      },
      "outputs": [],
      "source": [
        "dataset = './dataset/ielts-writing-essays.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx0erCPFj9Bg",
        "outputId": "94f2269e-118a-4a9f-bbe3-8dce25c4fe37"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Essay</th>\n",
              "      <th>Overall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Between 1995 and 2010, a study was conducted r...</td>\n",
              "      <td>5.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Poverty represents a worldwide crisis. It is t...</td>\n",
              "      <td>6.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The left chart shows the population change hap...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Human beings are facing many challenges nowada...</td>\n",
              "      <td>5.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Information about the thousands of visits from...</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Essay  Overall\n",
              "0  Between 1995 and 2010, a study was conducted r...      5.5\n",
              "1  Poverty represents a worldwide crisis. It is t...      6.5\n",
              "2  The left chart shows the population change hap...      5.0\n",
              "3  Human beings are facing many challenges nowada...      5.5\n",
              "4  Information about the thousands of visits from...      7.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load dataset & Prepare Dataframes\n",
        "df = pd.read_csv(dataset)\n",
        "display(df.head())\n",
        "sentences = pd.DataFrame(df['Essay'])\n",
        "scores = pd.DataFrame(df['Overall'])\n",
        "\n",
        "# Define functions\n",
        "def tokenize_and_filter_sentence(sentence):\n",
        "    tokenizer = enchant.tokenize.get_tokenizer(\"en_GB\")\n",
        "    tokens = list(tokenizer(sentence))\n",
        "    tokens = [token[0] for token in tokens]\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "def convert_to_sequence(sentence, word_index):\n",
        "    tokens = tokenize_and_filter_sentence(sentence)\n",
        "    return [word_index.get(token, word_index['<OOV>']) for token in tokens]\n",
        "\n",
        "def alternative_word2vec(sen, model):\n",
        "    return [model.wv[word] for word in sen if word in model.wv]\n",
        "\n",
        "def calculate_average_length(sentences):\n",
        "    total_len = 0\n",
        "    for sentence in sentences:\n",
        "        total_len += len(sentence)\n",
        "    average_len = int(round(total_len / len(sentences), 0))\n",
        "    return average_len\n",
        "\n",
        "def pad_word2vec(sentence_vectors, maxlen, vector_size):\n",
        "    sentence_length = len(sentence_vectors)\n",
        "    if sentence_length > maxlen:\n",
        "        return np.array(sentence_vectors[:maxlen])\n",
        "    else:\n",
        "        padding = [np.zeros(vector_size) for _ in range(maxlen - sentence_length)]\n",
        "        return np.array(sentence_vectors + padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m word_index \u001b[38;5;241m=\u001b[39m {word: idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(unique_tokens)}\n\u001b[0;32m      7\u001b[0m word_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 8\u001b[0m sequences \u001b[38;5;241m=\u001b[39m [convert_to_sequence(sentence, word_index) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m all_sentences]\n\u001b[0;32m     10\u001b[0m vector_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     11\u001b[0m maxlen \u001b[38;5;241m=\u001b[39m calculate_average_length(tokenized_sentences)\n",
            "Cell \u001b[1;32mIn[7], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m word_index \u001b[38;5;241m=\u001b[39m {word: idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(unique_tokens)}\n\u001b[0;32m      7\u001b[0m word_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 8\u001b[0m sequences \u001b[38;5;241m=\u001b[39m [\u001b[43mconvert_to_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m all_sentences]\n\u001b[0;32m     10\u001b[0m vector_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     11\u001b[0m maxlen \u001b[38;5;241m=\u001b[39m calculate_average_length(tokenized_sentences)\n",
            "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mconvert_to_sequence\u001b[1;34m(sentence, word_index)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_sequence\u001b[39m(sentence, word_index):\n\u001b[1;32m---> 16\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_and_filter_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [word_index\u001b[38;5;241m.\u001b[39mget(token, word_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
            "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mtokenize_and_filter_sentence\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_and_filter_sentence\u001b[39m(sentence):\n\u001b[1;32m----> 9\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43menchant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_GB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(tokenizer(sentence))\n\u001b[0;32m     11\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [token[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\enchant\\tokenize\\__init__.py:209\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[1;34m(tag, chunkers, filters)\u001b[0m\n\u001b[0;32m    207\u001b[0m tag \u001b[38;5;241m=\u001b[39m tag\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# First try the whole tag\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m tk_func \u001b[38;5;241m=\u001b[39m \u001b[43m_try_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tk_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# Try just the base\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     base \u001b[38;5;241m=\u001b[39m tag\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32mg:\\App\\Anaconda\\envs\\NLP\\lib\\site-packages\\enchant\\tokenize\\__init__.py:315\u001b[0m, in \u001b[0;36m_try_tokenizer\u001b[1;34m(mod_name)\u001b[0m\n\u001b[0;32m    313\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m mod_base \u001b[38;5;241m+\u001b[39m mod_name\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(mod, func_name)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:1544\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "M6HKoRxvic37"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim,hidden_dim, num_layers, dropout_rate, embedding_layer):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = embedding_layer\n",
        "        self.lstm = nn.LSTM(embedding_layer, hidden_dim, num_layers=num_layers, \n",
        "                    dropout=dropout_rate, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, src, hidden):\n",
        "        embedding = self.dropout(self.embedding(src))\n",
        "        output, hidden = self.lstm(embedding, hidden)          \n",
        "        output = self.dropout(output) \n",
        "        prediction = self.fc(output)\n",
        "        return prediction, hidden\n",
        "    \n",
        "    def init_weights(self):\n",
        "        init_range_other = 1/math.sqrt(self.hidden_dim)\n",
        "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
        "        self.fc.bias.data.zero_()\n",
        "        for i in range(self.num_layers):\n",
        "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
        "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
        "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n",
        "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return hidden, cell\n",
        "    \n",
        "    def detach_hidden(self, hidden):\n",
        "        hidden, cell = hidden\n",
        "        hidden = hidden.detach()\n",
        "        cell = cell.detach()\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PARAMS\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 1024             # 400 in the paper\n",
        "hidden_dim = 256                # 1150 in the paper\n",
        "num_layers = 3                   # 3 in the paper\n",
        "dropout_rate = 0.2                            \n",
        "lr = 1e-3\n",
        "input_layer = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJLEtd27sQIA",
        "outputId": "fddc2c7e-3173-4f5d-d112-f596e5340fc8"
      },
      "outputs": [],
      "source": [
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
